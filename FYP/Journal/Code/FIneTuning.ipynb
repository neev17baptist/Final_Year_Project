{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\neevb\\OneDrive\\Desktop\\new_data_final_year\\analysis\\Data_ETo_PM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TMEAN']=(df['TMIN']+df['TMAX'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['ET0_PM','DATE'], axis=1)\n",
    "y = df[\"ET0_PM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scaler = StandardScaler()\n",
    "# # X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Linear: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  4.7956048794614905e-06\n",
      "Mean Absolute Error:  0.0014864514073429\n",
      "R-2 score:  0.9999979229222028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)\n",
    "mae1 = mean_absolute_error(y_test, y_pred1)\n",
    "mse1 = mean_squared_error(y_test, y_pred1)\n",
    "r21 = r2_score(y_test, y_pred1)\n",
    "print(\"Mean Squared Error: \", mse1)\n",
    "print(\"Mean Absolute Error: \", mae1)\n",
    "print(\"R-2 score: \", r21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Ridge: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  4.703112541476051e-06\n",
      "Mean Absolute Error:  0.0014825400167915053\n",
      "R-2 score:  0.9999979629825886\n"
     ]
    }
   ],
   "source": [
    "model2 = Ridge(alpha=1.0)  # Adjust alpha as needed\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "mae2=mean_absolute_error(y_test,y_pred2)\n",
    "mse2=mean_squared_error(y_test,y_pred2)\n",
    "r22=r2_score(y_test,y_pred2)\n",
    "print(\"Mean Squared Error: \",mse2)\n",
    "print(\"Mean Absolute Error: \",mae2)\n",
    "print(\"R-2 score: \",r22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Lasso: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  0.00030235926545812953\n",
      "Mean Absolute Error:  0.013774753160955517\n",
      "R-2 score:  0.999869041813734\n"
     ]
    }
   ],
   "source": [
    "model3=Lasso(alpha=0.01)\n",
    "model3.fit(X_train, y_train)\n",
    "y_pred3 = model3.predict(X_test)\n",
    "mae3=mean_absolute_error(y_test,y_pred3)\n",
    "mse3=mean_squared_error(y_test,y_pred3)\n",
    "r23=r2_score(y_test,y_pred3)\n",
    "print(\"Mean Squared Error: \",mse3)\n",
    "print(\"Mean Absolute Error: \",mae3)\n",
    "print(\"R-2 score: \",r23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.9978147851297912\n",
      "Mean Squared Error: 0.004344090720673638\n",
      "Mean Absolute Error: 0.04556318698827701\n",
      "R-squared Score: 0.9981184825247802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a Decision Tree Regressor\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Create a Grid Search object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Use the best parameters to train the final model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the final model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Decision Tree: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  0.004330952850134008\n",
      "Mean Absolute Error:  0.04542600164036401\n",
      "R-2 score:  0.9981241728140944\n"
     ]
    }
   ],
   "source": [
    "model4 = DecisionTreeRegressor(\n",
    "    criterion='squared_error',  # You can also use 'mae'\n",
    "    max_depth=10,  # Adjust the maximum depth of the tree\n",
    "    min_samples_split=5,  # Minimum samples required to split a node\n",
    "    min_samples_leaf=2  # Minimum samples required in a leaf node Â  \n",
    "\n",
    ")\n",
    "model4.fit(X_train, y_train)\n",
    "y_pred4 = model4.predict(X_test)\n",
    "mae4=mean_absolute_error(y_test,y_pred4)\n",
    "mse4=mean_squared_error(y_test,y_pred4)\n",
    "r24=r2_score(y_test,y_pred4)\n",
    "print(\"Mean Squared Error: \",mse4)\n",
    "print(\"Mean Absolute Error: \",mae4)\n",
    "print(\"R-2 score: \",r24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 8, 'n_estimators': 148}\n",
      "0.99905460913678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(low=50, high=200),\n",
    "    'max_depth': randint(low=1, high=10),\n",
    "    'min_samples_split': randint(low=2, high=10),\n",
    "    'min_samples_leaf': randint(low=1, high=5)\n",
    "}\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create a Randomized Search object\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the Randomized Search to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1440 fits failed out of a total of 4320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "461 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "979 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.03389484 -0.03348523 -0.03234029 -0.03326621 -0.03507912 -0.03638534\n",
      " -0.03426089 -0.03421451 -0.04050714 -0.04029594 -0.03889236 -0.03922166\n",
      " -0.03752341 -0.03795538 -0.03641842 -0.03633578 -0.0361117  -0.03796244\n",
      " -0.03719718 -0.0373323  -0.04310984 -0.04216622 -0.0416911  -0.04218184\n",
      " -0.04489252 -0.0462538  -0.04528487 -0.04601708 -0.04489252 -0.0462538\n",
      " -0.04528487 -0.04601708 -0.04747755 -0.0488705  -0.0475365  -0.04786374\n",
      " -0.03389484 -0.03348523 -0.03234029 -0.03326621 -0.03507912 -0.03638534\n",
      " -0.03426089 -0.03421451 -0.04050714 -0.04029594 -0.03889236 -0.03922166\n",
      " -0.03752341 -0.03795538 -0.03641842 -0.03633578 -0.0361117  -0.03796244\n",
      " -0.03719718 -0.0373323  -0.04310984 -0.04216622 -0.0416911  -0.04218184\n",
      " -0.04489252 -0.0462538  -0.04528487 -0.04601708 -0.04489252 -0.0462538\n",
      " -0.04528487 -0.04601708 -0.04747755 -0.0488705  -0.0475365  -0.04786374\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.05751058 -0.05923519 -0.05712974 -0.05758019 -0.06144599 -0.0599083\n",
      " -0.05810281 -0.05841313 -0.06285366 -0.06275426 -0.06155544 -0.06135543\n",
      " -0.06193727 -0.06412135 -0.06111549 -0.05993593 -0.05801615 -0.05824624\n",
      " -0.05829635 -0.05949367 -0.06069401 -0.06244664 -0.06162611 -0.06172663\n",
      " -0.06355778 -0.06544182 -0.06307365 -0.06334605 -0.06355778 -0.06544182\n",
      " -0.06307365 -0.06334605 -0.0622988  -0.06511292 -0.06355041 -0.06487611\n",
      " -0.05751058 -0.05923519 -0.05712974 -0.05758019 -0.06144599 -0.0599083\n",
      " -0.05810281 -0.05841313 -0.06285366 -0.06275426 -0.06155544 -0.06135543\n",
      " -0.06193727 -0.06412135 -0.06111549 -0.05993593 -0.05801615 -0.05824624\n",
      " -0.05829635 -0.05949367 -0.06069401 -0.06244664 -0.06162611 -0.06172663\n",
      " -0.06355778 -0.06544182 -0.06307365 -0.06334605 -0.06355778 -0.06544182\n",
      " -0.06307365 -0.06334605 -0.0622988  -0.06511292 -0.06355041 -0.06487611\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.03372695 -0.03392957 -0.03313832 -0.03313433 -0.03700523 -0.03661882\n",
      " -0.03470335 -0.0343851  -0.04053564 -0.04107406 -0.03980614 -0.03957976\n",
      " -0.03805155 -0.03901772 -0.03718265 -0.0371383  -0.03718106 -0.03875265\n",
      " -0.03734596 -0.03763995 -0.04102835 -0.04126342 -0.04144607 -0.04181597\n",
      " -0.04472145 -0.04594215 -0.04517933 -0.04618483 -0.04472145 -0.04594215\n",
      " -0.04517933 -0.04618483 -0.0475746  -0.04905154 -0.04752208 -0.04790152\n",
      " -0.03372695 -0.03392957 -0.03313832 -0.03313433 -0.03700523 -0.03661882\n",
      " -0.03470335 -0.0343851  -0.04053564 -0.04107406 -0.03980614 -0.03957976\n",
      " -0.03805155 -0.03901772 -0.03718265 -0.0371383  -0.03718106 -0.03875265\n",
      " -0.03734596 -0.03763995 -0.04102835 -0.04126342 -0.04144607 -0.04181597\n",
      " -0.04472145 -0.04594215 -0.04517933 -0.04618483 -0.04472145 -0.04594215\n",
      " -0.04517933 -0.04618483 -0.0475746  -0.04905154 -0.04752208 -0.04790152\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.03378788 -0.03341975 -0.03231142 -0.03325389 -0.03507912 -0.03638534\n",
      " -0.03426089 -0.03421451 -0.04050714 -0.04029594 -0.03889236 -0.03922166\n",
      " -0.03752341 -0.03795538 -0.03641842 -0.03633585 -0.0361117  -0.03796244\n",
      " -0.03719718 -0.0373323  -0.04310984 -0.04216622 -0.0416911  -0.04218184\n",
      " -0.04489252 -0.0462538  -0.04528487 -0.04601708 -0.04489252 -0.0462538\n",
      " -0.04528487 -0.04601708 -0.04747755 -0.0488705  -0.0475365  -0.04786374\n",
      " -0.03378788 -0.03341975 -0.03231142 -0.03325389 -0.03507912 -0.03638534\n",
      " -0.03426089 -0.03421451 -0.04050714 -0.04029594 -0.03889236 -0.03922166\n",
      " -0.03752341 -0.03795538 -0.03641842 -0.03633585 -0.0361117  -0.03796244\n",
      " -0.03719718 -0.0373323  -0.04310984 -0.04216622 -0.0416911  -0.04218184\n",
      " -0.04489252 -0.0462538  -0.04528487 -0.04601708 -0.04489252 -0.0462538\n",
      " -0.04528487 -0.04601708 -0.04747755 -0.0488705  -0.0475365  -0.04786374\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.02818562 -0.02783867 -0.02699881 -0.02704086 -0.0283933  -0.02856934\n",
      " -0.02807886 -0.02815655 -0.03254349 -0.03276636 -0.03213085 -0.03183312\n",
      " -0.03160889 -0.03095461 -0.03006279 -0.03024482 -0.03212879 -0.03172239\n",
      " -0.03105396 -0.03083468 -0.033883   -0.03432601 -0.0338505  -0.03413316\n",
      " -0.03629775 -0.03769085 -0.03660853 -0.03675422 -0.03629775 -0.03769085\n",
      " -0.03660853 -0.03675422 -0.03919308 -0.03906854 -0.03782466 -0.03808862\n",
      " -0.02818562 -0.02783867 -0.02699881 -0.02704086 -0.0283933  -0.02856934\n",
      " -0.02807886 -0.02815655 -0.03254349 -0.03276636 -0.03213085 -0.03183312\n",
      " -0.03160889 -0.03095461 -0.03006279 -0.03024482 -0.03212879 -0.03172239\n",
      " -0.03105396 -0.03083468 -0.033883   -0.03432601 -0.0338505  -0.03413316\n",
      " -0.03629775 -0.03769085 -0.03660853 -0.03675422 -0.03629775 -0.03769085\n",
      " -0.03660853 -0.03675422 -0.03919308 -0.03906854 -0.03782466 -0.03808862\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.05502889 -0.05510747 -0.05389798 -0.05351459 -0.05500958 -0.05590626\n",
      " -0.055403   -0.05482947 -0.06019826 -0.05855026 -0.05732305 -0.05718543\n",
      " -0.05851999 -0.05874705 -0.05598924 -0.05547685 -0.05771659 -0.05751266\n",
      " -0.05588108 -0.0560866  -0.06002198 -0.06025852 -0.05917526 -0.05852813\n",
      " -0.05619011 -0.05832781 -0.05756628 -0.05789135 -0.05619011 -0.05832781\n",
      " -0.05756628 -0.05789135 -0.05713858 -0.0593466  -0.05854383 -0.0592082\n",
      " -0.05502889 -0.05510747 -0.05389798 -0.05351459 -0.05500958 -0.05590626\n",
      " -0.055403   -0.05482947 -0.06019826 -0.05855026 -0.05732305 -0.05718543\n",
      " -0.05851999 -0.05874705 -0.05598924 -0.05547685 -0.05771659 -0.05751266\n",
      " -0.05588108 -0.0560866  -0.06002198 -0.06025852 -0.05917526 -0.05852813\n",
      " -0.05619011 -0.05832781 -0.05756628 -0.05789135 -0.05619011 -0.05832781\n",
      " -0.05756628 -0.05789135 -0.05713858 -0.0593466  -0.05854383 -0.0592082\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.02923651 -0.02872461 -0.02714279 -0.02714331 -0.02888769 -0.02870691\n",
      " -0.02772042 -0.02833518 -0.03362366 -0.0335497  -0.0326414  -0.03198128\n",
      " -0.03133352 -0.03120718 -0.03030997 -0.03038674 -0.03182021 -0.03225744\n",
      " -0.03131069 -0.03095908 -0.03459165 -0.03556142 -0.03413891 -0.03431809\n",
      " -0.0367801  -0.03786728 -0.03662802 -0.03676081 -0.0367801  -0.03786728\n",
      " -0.03662802 -0.03676081 -0.03882999 -0.03894946 -0.0378116  -0.03820147\n",
      " -0.02923651 -0.02872461 -0.02714279 -0.02714331 -0.02888769 -0.02870691\n",
      " -0.02772042 -0.02833518 -0.03362366 -0.0335497  -0.0326414  -0.03198128\n",
      " -0.03133352 -0.03120718 -0.03030997 -0.03038674 -0.03182021 -0.03225744\n",
      " -0.03131069 -0.03095908 -0.03459165 -0.03556142 -0.03413891 -0.03431809\n",
      " -0.0367801  -0.03786728 -0.03662802 -0.03676081 -0.0367801  -0.03786728\n",
      " -0.03662802 -0.03676081 -0.03882999 -0.03894946 -0.0378116  -0.03820147\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.02789763 -0.02775679 -0.02705661 -0.02700215 -0.02839257 -0.02847441\n",
      " -0.02807773 -0.02815708 -0.03254349 -0.03276636 -0.03213085 -0.03183312\n",
      " -0.03160889 -0.03095005 -0.03006057 -0.03024173 -0.03212916 -0.03172254\n",
      " -0.03105405 -0.03083105 -0.033883   -0.03432601 -0.0338505  -0.03413316\n",
      " -0.03629775 -0.03769085 -0.03660853 -0.03675422 -0.03629775 -0.03769085\n",
      " -0.03660853 -0.03675422 -0.03919308 -0.03906854 -0.03782466 -0.03808862\n",
      " -0.02789763 -0.02775679 -0.02705661 -0.02700215 -0.02839257 -0.02847441\n",
      " -0.02807773 -0.02815708 -0.03254349 -0.03276636 -0.03213085 -0.03183312\n",
      " -0.03160889 -0.03095005 -0.03006057 -0.03024173 -0.03212916 -0.03172254\n",
      " -0.03105405 -0.03083105 -0.033883   -0.03432601 -0.0338505  -0.03413316\n",
      " -0.03629775 -0.03769085 -0.03660853 -0.03675422 -0.03629775 -0.03769085\n",
      " -0.03660853 -0.03675422 -0.03919308 -0.03906854 -0.03782466 -0.03808862]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define a comprehensive parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider for split\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV (exhaustive search over all combinations)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Use MSE for regression\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,  # Show progress\n",
    "    n_jobs=-1  # Use all available cores for parallel processing\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  0.0017373839798168163\n",
      "Mean Absolute Error:  0.02476083083677339\n",
      "R-2 score:  0.9992475022900338\n"
     ]
    }
   ],
   "source": [
    "model5 =RandomForestRegressor(\n",
    "bootstrap=False, max_depth=None,max_features='sqrt',min_samples_leaf=1,min_samples_split=2,n_estimators=200\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model5.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred5 = model5.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae5 = mean_absolute_error(y_test, y_pred5)\n",
    "mse5 = mean_squared_error(y_test, y_pred5)\n",
    "r25 = r2_score(y_test, y_pred5)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error: \", mse5)\n",
    "print(\"Mean Absolute Error: \", mae5)\n",
    "print(\"R-2 score: \", r25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "0.999689241466552\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Create a Grid Search object\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best Â  \n",
    "# parameters and score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0008385553931361603\n",
      "Mean Absolute Error: 0.021934917644513538\n",
      "R-squared Score: 0.9996368039418199\n"
     ]
    }
   ],
   "source": [
    "model6 = GradientBoostingRegressor(\n",
    "   learning_rate=0.1,max_depth=5,min_samples_leaf=1,min_samples_split=5,n_estimators=300\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model6.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred6 = model6.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae6 = mean_absolute_error(y_test, y_pred6)\n",
    "mse6 = mean_squared_error(y_test, y_pred6)\n",
    "r26 = r2_score(y_test, y_pred6)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse6)\n",
    "print(\"Mean Absolute Error:\", mae6)\n",
    "print(\"R-squared Score:\", r26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "0.9996333450791044\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear',\n",
    " 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'epsilon': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Create an SVR model\n",
    "svr = SVR()\n",
    "\n",
    "# Create a Grid Search object\n",
    "grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best Â  \n",
    "#  parameters and score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0010284989312415898\n",
      "Mean Absolute Error: 0.02602499535838756\n",
      "R-squared Score: 0.9995545353822455\n"
     ]
    }
   ],
   "source": [
    "model7 = SVR(kernel='linear', C=0.1, epsilon=0.1, gamma='scale')\n",
    "\n",
    "# Train the model on the training data\n",
    "model7.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred7 = model7.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae7 = mean_absolute_error(y_test, y_pred7)\n",
    "mse7 = mean_squared_error(y_test, y_pred7)\n",
    "r27 = r2_score(y_test, y_pred7)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse7)\n",
    "print(\"Mean Absolute Error:\", mae7)\n",
    "print(\"R-squared Score:\", r27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36864 candidates, totalling 184320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "4 fits failed out of a total of 184320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\sklearn.py\", line 1081, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\sklearn.py\", line 596, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\sklearn.py\", line 1003, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 1573, in __init__\n",
      "    self._init(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 1632, in _init\n",
      "    it.reraise()\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 569, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 550, in _handle_exception\n",
      "    return fn()\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 637, in <lambda>\n",
      "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 1402, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 626, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 954, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 1092, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 1348, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 679, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 1279, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [16:05:35] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\sklearn.py\", line 1081, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\sklearn.py\", line 596, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\sklearn.py\", line 1003, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 1573, in __init__\n",
      "    self._init(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 1632, in _init\n",
      "    it.reraise()\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 569, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 550, in _handle_exception\n",
      "    return fn()\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 637, in <lambda>\n",
      "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 1402, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 626, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 954, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 1092, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 1348, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 679, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\data.py\", line 1279, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [16:05:36] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\neevb\\anaconda3\\envs\\evdp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan ... -0.00520434 -0.00491649\n",
      " -0.00494682]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'reg_alpha': 0.01, 'reg_lambda': 0.01, 'subsample': 1.0}\n",
      "Test MSE: 0.001046110048142537\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Patch XGBRegressor to work with Scikit-Learn 2.1.1\n",
    "def _sklearn_tags(self):\n",
    "    return {\"non_deterministic\": True}\n",
    "\n",
    "XGBRegressor.__sklearn_tags__ = _sklearn_tags\n",
    "\n",
    "# Initialize XGBRegressor\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  \n",
    "    cv=5,  \n",
    "    verbose=2,\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.001046110048142537\n",
      "Mean Absolute Error: 0.01848139808308132\n",
      "R-squared Score: 0.9995469076354193\n"
     ]
    }
   ],
   "source": [
    "# model8 = from xgboost import XGBRegressor\n",
    "\n",
    "model8 = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0,\n",
    "    max_depth=5,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.01,\n",
    "    colsample_bytree=1.0,\n",
    "    subsample=1.0\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model on the training data\n",
    "model8.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred8 = model8.predict(X_test)\n",
    "\n",
    "# Evaluate Â  \n",
    "#  model performance\n",
    "mae8 = mean_absolute_error(y_test, y_pred8)\n",
    "mse8 = mean_squared_error(y_test, y_pred8) \n",
    "\n",
    "r28 = r2_score(y_test, y_pred8)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse8)\n",
    "print(\"Mean Absolute Error:\", mae8)\n",
    "print(\"R-squared Score:\", r28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 0.0001, 'l1_ratio': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.01, 0.1, 1.0, 10.0],  # Explore a wider range of alpha values\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # Explore a balanced range of l1_ratio values\n",
    "}\n",
    "\n",
    "# Create the ElasticNet model\n",
    "elastic_net = ElasticNet(random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Extract the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.6159491820061995e-06\n",
      "Mean Absolute Error: 0.0014727844667903542\n",
      "R-squared Score: 0.9999980007348812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "# Define the ElasticNet model with hyperparameters\n",
    "model9 = ElasticNet(alpha=0.0001, l1_ratio=0.9)  # Adjust alpha and l1_ratio as needed\n",
    "\n",
    "# Train the model on the training data\n",
    "model9.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred9 = model9.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae9 = mean_absolute_error(y_test, y_pred9)\n",
    "mse9 = mean_squared_error(y_test, y_pred9)\n",
    "r29 = r2_score(y_test, y_pred9)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse9)\n",
    "print(\"Mean Absolute Error:\", mae9)\n",
    "print(\"R-squared Score:\", r29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters found:  {'activation': 'relu', 'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'n_iter_no_change': 5, 'solver': 'adam', 'validation_fraction': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,)],\n",
    "    'activation': ['relu'],  # Experiment with other activations\n",
    "    'solver': ['adam'],  # Consider other solvers (sgd, lbfgs)\n",
    "    'alpha': [0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'early_stopping': [True],  # Enable early stopping\n",
    "    'validation_fraction': [0.1],  # Fraction of training data to set aside for validation\n",
    "    'n_iter_no_change': [5],  # Number of iterations with no change to wait for early stopping\n",
    "}\n",
    "\n",
    "# Create the MLPRegressor model\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0021528462615903227\n",
      "Mean Absolute Error: 0.03290806170090736\n",
      "R-squared Score: 0.9990675567977053\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model10 = MLPRegressor(hidden_layer_sizes=(100,),activation='relu', alpha= 0.01,early_stopping=True,learning_rate_init= 0.001,n_iter_no_change=5,solver='adam',validation_fraction= 0.1)\n",
    "\n",
    "# Train the model on the training data\n",
    "model10.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred10 = model10.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae10 = mean_absolute_error(y_test, y_pred10)\n",
    "mse10 = mean_squared_error(y_test, y_pred10)\n",
    "r210 = r2_score(y_test, y_pred10)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse10)\n",
    "print(\"Mean Absolute Error:\", mae10)\n",
    "print(\"R-squared Score:\", r210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neevb\\anaconda3\\envs\\evotra\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:19: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 7, 'max_iter': 300}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_iter': [100, 200, 300],\n",
    "}\n",
    "\n",
    "# Create a HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor()\n",
    "\n",
    "# Create a Grid Search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0011570307853411995\n",
      "Mean Absolute Error: 0.02216060678621396\n",
      "R-squared Score: 0.9994988655205503\n"
     ]
    }
   ],
   "source": [
    "model = HistGradientBoostingRegressor(max_depth=5, learning_rate=0.1, max_iter=100)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared Score:\", r2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
